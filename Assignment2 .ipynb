{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRbJQEMit-GH"
   },
   "source": [
    "# commands for gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "cPFckZsDGrsn",
    "outputId": "60fd36b3-fd55-48f2-cc27-8721c3a153e9"
   },
   "outputs": [],
   "source": [
    "!pip3 install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0hjQn5AGwwr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8trhvgeG4QQ"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxG966iNVta8"
   },
   "outputs": [],
   "source": [
    "download = drive.CreateFile({'id': '1nDlBKahqlM_0SgQQwBIgobR-JAwZ9deA'})\n",
    "download.GetContentFile('german_signs.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rQk0HtkpVuEl",
    "outputId": "15d72d12-81ba-40fc-faf8-86b88f6f59e5"
   },
   "outputs": [],
   "source": [
    "!unzip \"german_signs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ja2F8HkyHHUN",
    "outputId": "dc3c7848-aabb-41bc-9131-7a1fea88fe5d"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "u-3BHKBfHJ0O",
    "outputId": "a354a90c-1bde-430b-b574-578bb20c52aa"
   },
   "outputs": [],
   "source": [
    "!pip3 install natsort\n",
    "!pip3 install imutils\n",
    "# !pip3 install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iJ5xym64HL_G",
    "outputId": "9d2054cb-de9f-46f6-f588-c47366fcee4f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqgTjvKguCY8"
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8coHiambuGaS"
   },
   "source": [
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF37r4Y6Yvqm"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "import torch\n",
    "import PIL\n",
    "\n",
    "class CLAHE:\n",
    " def __init__(self, clipLimit=0.01, tileGridSize=(4, 4)):\n",
    "  self.clipLimit = clipLimit\n",
    "  self.tileGridSize = tileGridSize\n",
    "\n",
    " def __call__(self, im):\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "  #clahe = cv2.createCLAHE(clipLimit=self.clipLimit,\n",
    "  #tileGridSize=self.tileGridSize)\n",
    "  #im = clahe.apply(img_y)\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "  t = type(im)\n",
    "  img_y = exposure.equalize_adapthist(np.array(im))\n",
    "  #im = cv2.merge((img_y,cr,cb)) \n",
    "  return np.float32(img_y)#np.transpose(im,[2,0,1])\n",
    "\n",
    "# once the images are loaded, how do we pre-process them before being passed into the network\n",
    "# by default, we resize the images to 32 x 32 in size\n",
    "# and normalize them to mean = 0 and standard-deviation = 1 based on statistics collected from\n",
    "# the training set\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((48,48)),\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomApply([\n",
    "    #transforms.RandomRotation(20, resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(0, translate=(0.2, 0.2), resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(0, shear=20, resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(0, scale=(0.8, 1.2), resample=PIL.Image.BICUBIC)]),\n",
    "    #CLAHE(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "\n",
    "def initialize_data(folder):\n",
    "    train_zip = folder + '/train_images.zip'\n",
    "    test_zip = folder + '/test_images.zip'\n",
    "    if not os.path.exists(train_zip) or not os.path.exists(test_zip):\n",
    "        raise(RuntimeError(\"Could not find \" + train_zip + \" and \" + test_zip\n",
    "              + ', please download them from https://www.kaggle.com/c/nyu-cv-fall-2018/data '))\n",
    "    # extract train_data.zip to train_data\n",
    "    train_folder = folder + '/train_images'\n",
    "    if not os.path.isdir(train_folder):\n",
    "        print(train_folder + ' not found, extracting ' + train_zip)\n",
    "        zip_ref = zipfile.ZipFile(train_zip, 'r')\n",
    "        zip_ref.extractall(folder)\n",
    "        zip_ref.close()\n",
    "    # extract test_data.zip to test_data\n",
    "    test_folder = folder + '/test_images'\n",
    "    if not os.path.isdir(test_folder):\n",
    "        print(test_folder + ' not found, extracting ' + test_zip)\n",
    "        zip_ref = zipfile.ZipFile(test_zip, 'r')\n",
    "        zip_ref.extractall(folder)\n",
    "        zip_ref.close()\n",
    "\n",
    "    # make validation_data by using images 00000*, 00001* and 00002* in each class\n",
    "    train_folder = folder + '/augeq_train_images'\n",
    "    val_folder = folder + '/augeq_val_images'\n",
    "    if not os.path.isdir(val_folder):\n",
    "        print(val_folder + ' not found, making a validation set')\n",
    "        os.mkdir(val_folder)\n",
    "        for dirs in os.listdir(train_folder):\n",
    "            files = os.listdir(train_folder + '/' + dirs)\n",
    "            random_files = np.random.choice(files,int(0.2*len(files)),replace=False)\n",
    "            if not os.path.isdir(val_folder + '/' + dirs):\n",
    "                os.mkdir(val_folder + '/' + dirs)\n",
    "            \n",
    "            #if dirs.startswith('000'):\n",
    "             #   os.mkdir(val_folder + '/' + dirs)\n",
    "            for f in random_files:\n",
    "                print(train_folder + '/' + dirs + '/' + f,val_folder + '/' + dirs + '/' + f)  \n",
    "              #if f.startswith('00000') or f.startswith('00001') or f.startswith('00002'):\n",
    "                        # move file to validation folder\n",
    "                os.rename(train_folder + '/' + dirs + '/' + f, val_folder + '/' + dirs + '/' + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5TaLuReuLNL"
   },
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ius0pg4YiUs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nclasses = 43 # GTSRB as 43 classes\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,padding=(3,3))\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5,padding=(2,2))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3,padding=(1,1))\n",
    "#         self.conv4 = nn.Conv2d(256, 256, kernel_size=3,padding=(1,1))\n",
    "#         self.conv5 = nn.Conv2d(256, 256, kernel_size=3,padding=(1,1))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(256*6*6, 256)\n",
    "        self.fc2 = nn.Linear(256, nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.batchnorm1(self.conv1(x))), 2)\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.batchnorm2(self.conv2(x))), 2)\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.batchnorm3(self.conv3(x))), 2)\n",
    "\n",
    "        x = x.view(-1, 256*6*6)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.batchnorm4(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V_5uh8buQRB"
   },
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1x-yOveZbjf"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFJvYRjNuT3_"
   },
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h6ewFgMYWWco",
    "outputId": "55a0f435-af30-4f84-dad0-9ded8da96692"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "#import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "import warnings\n",
    "# from focalloss import *\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch GTSRB example')\n",
    "parser.add_argument('--data', type=str, default='data', metavar='D',\n",
    "                    help=\"folder where data is located. train_data.zip and test_data.zip need to be found in the folder\")\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--epochs', type=int, default=25, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.002, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "########## class weights\n",
    "def make_weights_for_balanced_classes(images, nclasses):                        \n",
    "    count = [0] * nclasses                                                      \n",
    "    for item in images:                                                         \n",
    "        count[item[1]] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(images)                                              \n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val[1]]                                  \n",
    "    return weight\n",
    "#########Gamma for focal loss:\n",
    "g = 1\n",
    "\n",
    "#from sklearn.utils import class_weights\n",
    "### Data Initialization and Loading\n",
    "# from data import initialize_data, data_transforms # data.py in the same folder\n",
    "# initialize_data(args.data) # extracts the zip files, makes a validation set\n",
    "\n",
    "dataset_train = datasets.ImageFolder(\"./data/\" + 'train_images', transform=data_transforms)\n",
    "#dataset_labels = [i[1] for i in dataset_train.imgs]\n",
    "#class_count = np.bincount(dataset_labels)\n",
    "#weights = 1 / np.array([class_count[y] for y in dataset_labels])\n",
    "#weights = make_weights_for_balanced_classes(dataset_train.imgs, 43)\n",
    "#weights = torch.DoubleTensor(weights)       \n",
    "#sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n",
    "#print(weights)\n",
    "#datasets.ImageFolder(args.data + '/train_images', transform=data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "#    datasets.ImageFolder(args.data + '/train_images',\n",
    " #                        transform=data_transforms),\n",
    "    dataset_train,\n",
    "    batch_size=64, shuffle=True, num_workers=12)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('./data' + '/val_images',\n",
    "                         transform=data_transforms),\n",
    "    batch_size=64, shuffle=False, num_workers=12)\n",
    "\n",
    "\n",
    "### Neural Network and Optimizer\n",
    "# We define neural net in model.py so that it can be reused by the evaluate.py script\n",
    "# from model import Net\n",
    "model = Net()\n",
    "#model = torchvision.models.resnet18(pretrained=False, progress=True)\n",
    "#model = torchvision.models.vgg16_bn(pretrained=False, progress=True)\n",
    "model.cuda()\n",
    "summary(model, (3, 48, 48))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #print(data.shape,target.shape)\n",
    "        #print(np.bincount(target))\n",
    "        #for i in range(data.shape[0]):\n",
    "           # print(data[i].shape)#,np.max(data[i]),np.min(data[i]))\n",
    "        #   with warnings.catch_warnings():\n",
    "         #      warnings.simplefilter(\"ignore\")\n",
    "            #y,cb,cr = cv2.split(cv2.cvtColor(data[i], cv2.COLOR_BGR2YCR_CB))\n",
    "            #clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4,4))\n",
    "            #y = clahe.apply(y)\n",
    "            #data[i] = cv2.merge((y,cb,cr))#exposure.equalize_adapthist(data[i])\n",
    "          #  data[i] = exposure.equalize_adapthist(data[i])    \n",
    "       #print_progress(i + 1, X.shape[0])\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)#FocalLoss(gamma=g)(output,target)#F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.data.max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAccuracy {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), (100. * correct / 64)))\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "    #    torch.cuda.empty_cache()\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for data, target in val_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        #y_true.append(torch.argmax(target,-1))\n",
    "        #for i in range(data.shape[0]):\n",
    "            #y,cb,cr = cv2.split(cv2.cvtColor(data[i], cv2.COLOR_BGR2YCR_CB))\n",
    "            #clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4,4))\n",
    "            #y = clahe.apply(y)\n",
    "            #data[i] = cv2.merge((y,cb,cr))\n",
    "           #with warnings.catch_warnings():\n",
    "           #    warnings.simplefilter(\"ignore\")\n",
    "         #   data[i] = exposure.equalize_adapthist(data[i])\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        #op = model(data)\n",
    "        #y_pred.append(torch.argmax(output,-1))\n",
    "        validation_loss += F.nll_loss(output, target, size_average=False).item()#.cpu()#FocalLoss(gamma = g)(output,target)#F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]#.cpu() # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "    #print(100. * correct / len(val_loader.dataset))\n",
    "    acc = 100. * correct / len(val_loader.dataset)\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        acc))\n",
    "    if epoch == 100:\n",
    "        print(epoch)\n",
    "    return acc,validation_loss\n",
    "        #print(confusion_matrix(y_true,y_pred))        \n",
    "\n",
    "val_loss = 200\n",
    "val_acc = 0\n",
    "lossp = []\n",
    "valp = []\n",
    "for epoch in range(1, 200 + 1):\n",
    "    epoch_loss = train(epoch)\n",
    "    lossp.append(epoch_loss)\n",
    "    va,vl = validation()\n",
    "    valp.append(va.item())\n",
    "    scheduler.step(va)\n",
    "    print(va,vl)\n",
    "    if va > val_acc:\n",
    "        val_acc = va\n",
    "        model_file = 'model1_acc_' + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        print('\\nSaved Best acc model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')  \n",
    "        \n",
    "    if vl < val_loss:\n",
    "        val_loss = vl\n",
    "        model_file = 'model1_loss_' + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        print('\\nSaved Best loss model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pov1bto1uWxz"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "OyS7Jo1dpezY",
    "outputId": "12d84c82-7c19-41d5-ef04-feda20e96cea"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossp)\n",
    "plt.title('Training Loss Vs epoch')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('training loss')\n",
    "plt.show()\n",
    "plt.plot(valp)\n",
    "plt.title('Validation Accuracy Vs epoch')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRNYX-B7ubd4"
   },
   "source": [
    "## evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "lTUqPRHTl63w",
    "outputId": "120c627e-2e43-457b-8a73-946f3ff07024"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "# from data import initialize_data # data.py in the same folder\n",
    "# from model import Net\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GTSRB evaluation script')\n",
    "parser.add_argument('--data', type=str, default='data', metavar='D',\n",
    "                    help=\"folder where data is located. train_data.zip and test_data.zip need to be found in the folder\")\n",
    "parser.add_argument('--model', type=str, metavar='M',\n",
    "                    help=\"the model file to be evaluated. Usually it is of the form model_X.pth\")\n",
    "parser.add_argument('--outfile', type=str, default='gtsrb_kaggle.csv', metavar='D',\n",
    "                    help=\"name of the output csv file\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "state_dict = torch.load(\"./model1_acc_75.pth\")\n",
    "model = Net()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# from data import data_transforms\n",
    "\n",
    "class CLAHE:\n",
    "    def __init__(self, clipLimit=0.01, tileGridSize=(4, 4)):\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self, im):\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "  #clahe = cv2.createCLAHE(clipLimit=self.clipLimit,\n",
    "  #tileGridSize=self.tileGridSize)\n",
    "  #im = clahe.apply(img_y)\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "        t = type(im)\n",
    "        img_y = exposure.equalize_adapthist(np.array(im))\n",
    "     #   print(img_y.shape)\n",
    "        b,g,r = cv2.split(img_y)\n",
    "        img_y = cv2.merge((r,g,b))\n",
    "  #im = cv2.merge((img_y,cr,cb)) \n",
    "        return np.float32(img_y)#np.transpose(im,[2,0,1])\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    #CLAHE(),\n",
    "    transforms.Resize((48,48)),\n",
    "    #transforms.ToPILImage(),\n",
    "   # data_transforms = transforms.Compose([\n",
    "    #transforms.Resize((48, 48)),\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomApply([\n",
    "    #transforms.RandomRotation(degrees = 20, resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(degrees = 0, translate=(0.2, 0.2), resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(degrees = 0, shear=20, resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(degrees = 0, scale=(0.8, 1.2), resample=PIL.Image.BICUBIC)]),\n",
    "    #CLAHE(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "test_dir = \"./data\" + '/test_images'\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "output_file = open(\"gtsrb_kaggle.csv\", \"w\")\n",
    "output_file.write(\"Filename,ClassId\\n\")\n",
    "for f in tqdm(os.listdir(test_dir)):\n",
    "    if 'ppm' in f:\n",
    "        data = data_transforms(pil_loader(test_dir + '/' + f))\n",
    "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        file_id = f[0:5]\n",
    "        output_file.write(\"%s,%d\\n\" % (file_id, pred))\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "print(\"Succesfully wrote \" + 'gtsrb_kaggle.csv' + ', you can upload this file to the kaggle '\n",
    "      'competition at https://www.kaggle.com/c/nyu-cv-fall-2018/')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-z-C1RanFIq"
   },
   "outputs": [],
   "source": [
    "!cp \"./gtsrb_kaggle.csv\" \"/content/gdrive/My Drive/german_signs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zwCnQe_rx4w8"
   },
   "source": [
    "## Augment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CFloy_onjr0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "class CLAHE:\n",
    "    def __init__(self, clipLimit=0.01, tileGridSize=(4, 4)):\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self, im):\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "  #clahe = cv2.createCLAHE(clipLimit=self.clipLimit,\n",
    "  #tileGridSize=self.tileGridSize)\n",
    "  #im = clahe.apply(img_y)\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "        t = type(im)\n",
    "        img_y = exposure.equalize_adapthist(np.array(im))\n",
    "  #im = cv2.merge((img_y,cr,cb)) \n",
    "        return np.float32(img_y)#np.transpose(im,[2,0,1])\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    #CLAHE(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    #transforms.ToPILImage(),\n",
    "   # data_transforms = transforms.Compose([\n",
    "    #transforms.Resize((48, 48)),\n",
    "    #transforms.ToPILImage()\n",
    "    transforms.RandomApply([\n",
    "    transforms.RandomRotation(degrees = 20),\n",
    "    transforms.RandomAffine(degrees = 0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees = 0, shear=20),\n",
    "    transforms.RandomAffine(degrees = 0, scale=(0.8, 0.8))]),\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "data_transforms1 = transforms.Compose([\n",
    "    #CLAHE(),\n",
    "    transforms.Resize((48, 48)),\n",
    "   # transforms.ToPILImage(),\n",
    "    #transforms.RandomApply([\n",
    "    #transforms.RandomRotation(20, resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(0, translate=(0.2, 0.2), resample=PIL.Image.BICUBIC),\n",
    "    #transforms.RandomAffine(0, shear=20, resample=PIL.Image.BICUBIC),\n",
    "   # transforms.RandomAffine(0, scale=(0.8, 1.2), resample=PIL.Image.BICUBIC)]),\n",
    "  #  CLAHE(),\n",
    "    transforms.ToTensor(),\n",
    " #   transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "def make_weights_for_balanced_classes(images, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in images:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(images)\n",
    "    for idx, val in enumerate(images):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "dataset_train = datasets.ImageFolder(\"./data\" + '/train_images', transform=data_transforms)\n",
    "dataset_train_orig = datasets.ImageFolder(\"./data\" + '/train_images', transform=data_transforms1)\n",
    "#dataset_train1 = torch.utils.data.ConcatDataset((dataset_train_aug,dataset_train_orig))\n",
    "#class CLAHE:\n",
    "    #def __init__(self, clipLimit=0.01, tileGridSize=(4, 4)):\n",
    "     #   self.clipLimit = clipLimit\n",
    "      #  self.tileGridSize = tileGridSize\n",
    "\n",
    "    #def __call__(self, im):\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "  #clahe = cv2.createCLAHE(clipLimit=self.clipLimit,\n",
    "  #tileGridSize=self.tileGridSize)\n",
    "  #im = clahe.apply(img_y)\n",
    "  #img_y,cr,cb = cv2.split(cv2.cvtColor(np.transpose(np.float32(im),[1,2,0]), cv2.COLOR_RGB2YCrCb))\n",
    "       # t = type(im)\n",
    "       # img_y = exposure.equalize_adapthist(np.array(im))\n",
    "#dataset_labels = [i[1] for i in dataset_train.imgs]\n",
    "#class_count = np.bincount(dataset_labels)\n",
    "#weights = 1 / np.array([class_count[y] for y in dataset_labels])\n",
    "weights = make_weights_for_balanced_classes(dataset_train.imgs, 43)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 43*3000)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "#    datasets.ImageFolder(args.data + '/train_images',\n",
    " #                        transform=data_transforms),\n",
    "    torch.utils.data.ConcatDataset((dataset_train_orig,dataset_train)),\n",
    "    batch_size=64, sampler = sampler, num_workers=4)\n",
    "\n",
    "for i,(image,label) in enumerate(train_loader):\n",
    "    image = image.data.numpy()\n",
    "    print(i)\n",
    "    label = label.data.numpy()\n",
    "    for j in range(image.shape[0]):\n",
    "        if not os.path.isdir(\"./data/augeq_train_images/\" + str(label[j]) + \"/\"):\n",
    "            os.mkdir(\"./data/augeq_train_images/\" + str(label[j]) + \"/\")\n",
    "        image1 = np.transpose(image[j],[1,2,0])\n",
    "#         print(np.unique(image))\n",
    "#         plt.imshow(image1)\n",
    "#         plt.show()\n",
    "        cv2.imwrite(\"./data/augeq_train_images/\"+ str(label[j]) + \"/\"+str(i+j)+\".ppm\",image1*255)\n",
    "                                                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqP23amKrSkw"
   },
   "source": [
    "# Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tl0YUstrS7j"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "1) Tried a basic model consisting of 3 convolutional layers all with 3x3 filters and two dense layers including the output. \n",
    "2) Referenced and experimented with 7x7 and 5x5 filters which worked better. Finally 7x7 for the first layer , 5x5 for the second and 3x3 for the third convolutional layer respectively work best.\n",
    "3) Included batch normalization after each layer. Gave better val accuracy\n",
    "4) Flatten vs Global Average Pooling. Gave better val accuracy with flattening the final vector\n",
    "5) Tried to use Focal loss with various gamma values to handle classifying hard classes (in this case classes having low number of images). Gamma = 1 worked best.\n",
    "6) Experimented with the number of neurons in the last before dense layer. 256 neurons worked best.\n",
    "7) Dropouts after conv layers did not work well (does not make sense but many papers use this). However dropouts after dense layers seem to increase the accuracy.\n",
    "8) Tried to train on the given dataset and then finetune on a manually created balanced set containing equal number of images (Least number of images in all classes will be the number of images per class). It did not improve accuracy.\n",
    "9) Added a learning rate scheduler which would reduce the learning rate when validation accuracy flattens out.\n",
    "10) Saved the models only if val accuracy or loss increases or decreases respectively.\n",
    "11) Experimented with various input sizes (48 and 32) 48 gave the best accuracy.\n",
    "12) Created an augmented dataset consisting of 1200 images per class using randomly applying rotation, shearing, translation or scaling to the image. Used a balanced random sampler to achieve this using class weights.\n",
    "12a) Used Contrast limited adaptive histogram equalization for reducing illumination variation over the images.\n",
    "12b) Trained on the augmented dataset using the same network and achieved 99% accuracy on the val set. However, due to some errors while transforming, the test set accuracy was below 0.01 which was fishy and needs debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EterqrbBraem"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2al_-Iirao6"
   },
   "outputs": [],
   "source": [
    "1) J. Stallkamp, M. Schlipsing, J. Salmen, C. Igel,\n",
    "   Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition, Neural Networks, Volume 32, 2012, Pages 323-332, ISSN 0893-6080,\n",
    "   https://doi.org/10.1016/j.neunet.2012.02.016. (http://www.sciencedirect.com/science/article/pii/S0893608012000457)\n",
    "2) P. Sermanet and Y. LeCun, \"Traffic sign recognition with multi-scale Convolutional Networks,\" The 2011 International Joint Conference on Neural Networks, San Jose, CA, 2011, pp. 2809-2813.\n",
    "   doi: 10.1109/IJCNN.2011.6033589\n",
    "   keywords: {computer vision;image classification;image colour analysis;traffic engineering computing;traffic sign recognition;multiscale convolutional network;traffic sign classification;GTSRB competition;multistage architecture;hierarchy learning;vision approach;hand-crafted features;HOG;SIFT;greyscale images;Image color analysis;Accuracy;Training;Color;Feature extraction;Neural networks;Computer architecture},\n",
    "   URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6033589&isnumber=6033131\n",
    "3) \\bibitem[Lin et al.(2017)]{2017arXiv170802002L} Lin, T.-Y., Goyal, P., Girshick, R., et al.\\ 2017, arXiv e-prints, arXiv:1708.02002\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
